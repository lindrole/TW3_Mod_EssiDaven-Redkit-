radish Lipsynced Animation Generation for REDkit
------------------------------------------------------------------

This project template supports two usecases for existing REDkit projects:

   a) generation of lip animations synchronized with prerecorded audio

   b) generation of lip animations from text only (no audio available)

Read the relevant subsection step by step guide. Also make sure to read the last
section with some additonal info.

IMPORTANT:

At the time of this writing scenes in the REDkit (PATCH 3) scene editor do not 
link dialoglines to existing audio and animation files automatically. This has 
to be done manually in the editor for every scene via a menu option (but only 
once), see below.

------------------------------------------------------------------
HOWTO generate lipsync animations from audio
------------------------------------------------------------------

Requirements:
   - exported REDkit project strings csv
   - recorded voiceover audio files as 44.1 kHz or 48 kHz ogg or wav (preferable)
     files
   - voiceover must match exactly the textlines from the csv
   - installed Wwise (v2021.1.14)

0. unpack/copy the content of the template.project folder into a new directory

1. open _settings_.bat in a text editor, read *all* the information below the
   "check these settings" line and adjust the following directory settings
   according to your local installation:

   - DIR_RADISH_ENCODER
  
   - DIR_REDKIT_PROJECT_ROOT

   - DIR_WWISE_BIN_NEXT_GEN

2. export the REDkit strings as csv and place the file into the root folder of
   the lipsync project:

      <your lipsync project>/
         LocalEditorStringDataBaseW3_UTF8_mod_export.csv

3. put all audio files into the appropriate <lang>.audio subdirectory, eg:

      <your lipsync project>/
         en.audio/
            *.wav
         fr.audio/
            *.wav

   Currently the following languages are supported: de, en, fr, pl, ru

4. link audio files with textlines:

   The lipsync build pipeline expects the audio filenames to be prefixed with
   the stringid of the textline. Either rename the audio files manually or use
   the phoneme extractor gui to match audio files to textlines (see [1]). The
   gui can be started with "adjust.audio.timings.bat". Then open the directory
   with the audio files via the menu option.
   Note: The textline selection dialog search filter will be preset with the
   filename. So if the filename already starts with the beginning of the
   textline the search result list will be very short.

5. start the generate.lip.sync.from.audio.bat file

   You should see a window with log output as the timings are extracted and the
   animation is generated. Make sure to read the final lines of the output to
   ensure no errors occurred. You can also examine the full build log file in 
   the project root directory.

   After a successfull generation there should be a "speech" folder in your 
   REDkit project directory containing the generated files:

     <your REDkit project>/
        speech/
           <lang>/
              audio/
                 *.wem             <- encoded audio used in the game
              audio_original/
                 *.wav             <- only if wav files were used (ogg files are not supported by REDkit)
              lipsync/
                 *.re              <- generated lipsync animations
           <lang2>
              ...

6. The audio and animations can now be used in the REDkit scene editor but 
   require linking with the scene lines of the scene. Open a scene in the 
   REDkit scene editor and generate the linkage via the menu option:

     Tool/Generate default audio paths...

   The scene playback should now use the imported audio and lipsynced animations.

[1] https://wiki.nexusmods.com/images/4/4c/T6.2019-09-04_id.selection.mp4

------------------------------------------------------------------
HOWTO generate lipsync animations from text only
------------------------------------------------------------------

Requirements:
   - exported REDkit project strings csv

0. unpack/copy the content of the template.project folder into a new directory

1. open _settings_.bat in a text editor, read *all* the information below the
   "check these settings" line and adjust the following settings according to
   your local installation:

   - DIR_RADISH_ENCODER

   - DIR_REDKIT_PROJECT_ROOT

   - set CONVERT_TO_WEM to no

   - (optional) set CREATE_W3SPEECH_FILES to yes and adjust DIR_W3SPEECH

2. export the REDkit strings as csv and place the file into the root folder of
   the lipsync project:

      <your lipsync project>/
         LocalEditorStringDataBaseW3_UTF8_mod_export.csv

3. create an empty <lang>.audio directory for all the languages that you want to
   generate language specific animations, like:

      <your lipsync project>/
         en.audio/
         fr.audio/
         ...

   Currently the following languages are supported: de, en, fr, pl, ru

4. start the generate.lip.animations.from.text.bat file

   You should see a window with log output as the animation is generated. Make 
   sure to read the final lines of the output to ensure no errors occurred. You 
   can also examine the full build log file in the project root directory.

   After a successfull generation there should be a "speech" folder in your 
   REDkit project directory containing the generated files:

     <your REDkit project>/
        speech/
           <lang>/
              audio/
                 *.wem             <- silent placeholder encoded audio used in the game
              lipsync/
                 *.re              <- generated lipsync animations
           <lang2>
              ...

   If CREATE_W3SPEECH_FILES was set to "yes" additional language dependent 
   w3speech files should be in the target directory DIR_W3SPEECH.

5. The (silent) audio and animations can now be used in the REDkit scene editor 
   but require linking with the scene lines of the scene. Open a scene in the 
   REDkit scene editor and generate the linkage via the menu option:

     Tool/Generate default audio paths...

   The scene playback should now use the imported lipsynced animations.
   
------------------------------------------------------------------
Additional Info / Notes
------------------------------------------------------------------

- make sure to export the REDkit csv right before generating the lipsync 
  animations to work with the latest version! 

- different head mesh files in the game are setup with different sets of 
  poseweights which may result in more or less different playback of the lipsync 
  animations. It is possible to select specific lipsync animation profiles per 
  actor from a set of predefined animation profiles. This is possible in the 
  phoneme extractor GUI or directly in the actor_mapping.cfg file. The first 
  generate.lip.*.bat run will update the config with all processed actors and 
  map (some of the) known vanilla actors to their dedicated profiles. You can 
  (and probably should) experiment with different profiles for custom actors and 
  decide which is fitting best based on the animation results.

  Note: the animation generation profiles are derived from the poseweights 
  provided in the blender plugin from REDkit patch 3.

- renaming the scenefile does not update the strings database containing the 
  references to voiceover and lipsync animation files (as embedded within the 
  scene file). it may be necessary to open and resave the scene and reexport the 
  csv file.

- it is important that the text matches the audio exactly. It *may* be necessary
  to manually edit the csv file prior to timing extraction/anim generation:
   - to expand certain abbrevations (e.g. numbers),
   - remove characters that are not meant to be spoken but were automatically
     expanded (e.g. * -> "asterisk") or
   - tweak parts of the text (e.g. "hmmm...") because they were not recognized
     by the phoneme translater and spelled character by character (e.g.
     hmmm -> eɪtʃˌɛmˈɛmˌɛm)

- check the log / phoneme extractor GUI for warnings and/or errors for the
  voiceover lines. There are some heuristic in the GUI that try to detect
  problems automatically (e.g. unusually long/short phoneme timings, gaps in
  words, etc.) Decide if it is necessary to adjust the timings/and or input text.

- generated w3speech files are completely independent of the REDkit project, can
  be used by the game directly and cannot be modified by any REDkit editor:
  any changes in REDkit scene editor will not be reflected in the w3speech files!

- extracted timings are stored in <id>.phoneme text files in the <lang>.audio
  directory. if a phoneme file for an audio file already exists the extraction
  step is skipped and the stored info is used. this ensures any subsequent
  adjustments in the gui are not overriden.

- after any manual adjustments of timings a full generate run must be started to
  update the animations (aka starting one of the generate batch files).

- if you want to reset/redo the automatic timing extraction for a specific audio
  file (e.g. because the text/and or audio file changed) you need to delete the
  appropriate phoneme file and rerun the appropriate generate batch file.

- the timing extraction from audio is language specific. Make sure text language
  in the exported REDkit strings csv matches the languages of its column.
  Otherwise the timing extraction results will be *very* bad or fail completely.

- do NOT generate from audio AND generate from text in the same project folder.
  it will create additional placeholder audio files and there will be two files
  for the same id which will lead to unexpected results. If you really want both
  versions use a different, clean project directory.

- directory paths containing a closing ) bracket are a pain in batch scripts. If
  any of your paths does contain such a brakcet, e.g.

      c:\Program Files (x86)\my directory

  it has to be escaped with a ^ character in the _settings_.bat file:

      c:\Program Files (x86^)\my directory

  Note: if there are still problems with the generation and the error messages
  contain a part of a defined DIR_* setting it's probably easier to copy/move
  the data and use paths that do not contain any brackets and spaces.

  Note: do NOT use a path with brackets for the lipsync project template! Even
  with escaped brackets it will NOT work!

- it's possible to configure (expand or restrict) the set of languages that are 
  scanned for generation via the LANGEUAGE variable in the _settings_.bat 
  config. However in order to actually generate lip animations a 
  <language-code>.audio directory must also be present within the lipsync 
  generation project directory.

  Note: language specific processing requires build-in language support in the 
  bundled espeak library *and* the appropriate phoneme recognitions models (for 
  pocketsphinx) for generating lipsynced animations from audio. Check the 
  nexusmods page for additional drop-in audio support packages (de, en, fr, pl 
  and ru will be provided). 

  However if you want to generate language specific lip animations from text 
  it's sufficient if the language is supported in espeak (unfortunately this 
  version does NOT support Arabic, Japanese or simplified Chinese).

- it's possible some "no phoneme mapping in repository for ..." warnings will 
  appear in the animation generation step (especially for additional languages 
  added in the _settings_ LANGUAGE variable). This means there is no 
  preconfigured animation preset for this phoneme, yet. The easiest way is to 
  define an alias for this phoneme and point to an existing similar sounding
  phoneme in the phoneme.alias.repo.yml config in one or all of the generation
  profiles in <DIR_RADISH_ENCODER>/repo.lipsync/*.lipsync/ directories. Make 
  sure to copy the exact phoneme character(s) from the log into the config.  

------------------------------------------------------------------

Have fun,
rmemr
